\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{DBLP:journals/corr/HuangLW16a}
\citation{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}
\citation{LeCun:1989:BAH:1351079.1351090}
\citation{Lecun98gradient-basedlearning}
\citation{DBLP:journals/corr/HeZRS15}
\citation{DBLP:journals/corr/HeZRS15}
\citation{NIPS2015_5850}
\citation{DBLP:journals/corr/LarssonMS16a}
\citation{DBLP:journals/corr/HuangLW16a}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A 5-layer dense block with a growth rate of k = 4. Each layer takes all preceding feature-maps as input.}}{1}{figure.1}}
\newlabel{fig: image1.png}{{1}{1}{A 5-layer dense block with a growth rate of k = 4. Each layer takes all preceding feature-maps as input}{figure.1}{}}
\citation{DBLP:journals/corr/HeZRS15}
\citation{CIFAR10}
\citation{cifar100}
\citation{37648}
\citation{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14}
\citation{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}
\citation{DBLP:journals/corr/HeZRS15}
\citation{DBLP:journals/corr/SzegedyLJSRAEVR14}
\citation{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14}
\citation{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A deep DenseNet with three dense blocks. The layers between two adjacent blocks are referred to as transition layers and change the feature-map sizes via convolution and pooling.}}{2}{figure.2}}
\newlabel{fig: image_large.png}{{2}{2}{A deep DenseNet with three dense blocks. The layers between two adjacent blocks are referred to as transition layers and change the feature-map sizes via convolution and pooling}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}DenseNet}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Experiments}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Performance Evaluation}{2}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The architecture has 2 Transition Up (TU) blocks an 2 Transition Down (TD) blocks and gray arrows represent the skip connections from TU blocks to TD blocks.}}{2}{figure.3}}
\newlabel{fig: image_tiramisu.png}{{3}{2}{The architecture has 2 Transition Up (TU) blocks an 2 Transition Down (TD) blocks and gray arrows represent the skip connections from TU blocks to TD blocks}{figure.3}{}}
\citation{Lafferty:2001:CRF:645530.655813}
\citation{DBLP:journals/corr/JegouDVRB16}
\citation{BrostowSFC:ECCV08}
\citation{DBLP:journals/corr/PleissCHLMW17}
\citation{DBLP:journals/corr/JegouDVRB16}
\bibstyle{ieee}
\bibdata{egbib}
\bibcite{BrostowSFC:ECCV08}{1}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Accuracy of different model architectures on Dog-Breed Identification task.}}{3}{table.1}}
\newlabel{table: table1}{{1}{3}{Accuracy of different model architectures on Dog-Breed Identification task}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Accuracy of different model architectures on Dog-Breed Identification task without data augmentation.}}{3}{table.2}}
\newlabel{table: table2}{{2}{3}{Accuracy of different model architectures on Dog-Breed Identification task without data augmentation}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Memory Consumption}{3}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Training Time}{3}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Conclusion}{3}{section.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Future Scope}{3}{section.5}}
\bibcite{DBLP:journals/corr/HeZRS15}{2}
\bibcite{DBLP:journals/corr/HuangLW16a}{3}
\bibcite{DBLP:journals/corr/JegouDVRB16}{4}
\bibcite{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}{5}
\bibcite{CIFAR10}{6}
\bibcite{cifar100}{7}
\bibcite{Lafferty:2001:CRF:645530.655813}{8}
\bibcite{DBLP:journals/corr/LarssonMS16a}{9}
\bibcite{LeCun:1989:BAH:1351079.1351090}{10}
\bibcite{Lecun98gradient-basedlearning}{11}
\bibcite{37648}{12}
\bibcite{DBLP:journals/corr/PleissCHLMW17}{13}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Original DenseNet implementation is on the left and its memory efficient implementation is on the right. The memory efficient implementation stores the output of the concatenation, batch normalization, and ReLU layers in temporary storage buffers, whereas the original implementation allocates new memory.}}{4}{figure.4}}
\newlabel{fig: image_memory.png}{{4}{4}{Original DenseNet implementation is on the left and its memory efficient implementation is on the right. The memory efficient implementation stores the output of the concatenation, batch normalization, and ReLU layers in temporary storage buffers, whereas the original implementation allocates new memory}{figure.4}{}}
\bibcite{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14}{14}
\bibcite{NIPS2015_5850}{15}
\bibcite{DBLP:journals/corr/SzegedyLJSRAEVR14}{16}
